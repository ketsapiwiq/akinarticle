{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import URIs\n",
    "2. Import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-72f5527500cd>, line 5)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-72f5527500cd>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    while IFS= read -r line\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#!/usr/local/bin/bash\n",
    "\n",
    "input=\"../bafe/bafe_articles.uris\"\n",
    "\n",
    "while IFS= read -r line\n",
    "do\n",
    "    outFile=$(echo \"$line\" | cut -d /  -f 5- | tr / _)\n",
    "    curl $line -o $outFile.html\n",
    "    pandoc -f html -t markdown $outFile.html -o $outFile.md\n",
    "done < \"$input\"\n",
    "# curl $1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'readabilipy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-93842131850d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreadabilipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimple_json_from_html_string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0monlyhtmlfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'readabilipy'"
     ]
    }
   ],
   "source": [
    "\n",
    "#  simple_json.py file containing the function simple_json_from_html_string().\n",
    "#     Usage:\n",
    "\n",
    "mypath=\"data/articles/bafe\"\n",
    "\n",
    "\n",
    "from os import *\n",
    "from os.path import isfile, join\n",
    "onlyhtmlfiles = [f for f in listdir(mypath) if (isfile(join(mypath, f)) and f[:5] == \".html\")]\n",
    "\n",
    "\n",
    "from readabilipy import simple_json_from_html_string\n",
    "\n",
    "for file in onlyhtmlfiles:\n",
    "    with os.open(file) as html_file:\n",
    "        html_strings = readlines(html_file)\n",
    "        article = simple_json_from_html_string(html_strings, content_digests=False, node_indexes=True, use_readability=True)\n",
    "        with open('a', file+\".json\") as json_file:\n",
    "            write(json_file, article)\n",
    "\n",
    "        #The function returns a dictionary with the following fields:\n",
    "         #   title: The article title\n",
    "         #   byline: Author information\n",
    "         #   content: A simplified HTML representation of the article, with all article text contained in paragraph elements.\n",
    "         #   plain_content: A \"plain\" version of the simplified Readability.js article HTML present in the content field. This attempts to retain only the plain text content of the article, while preserving the HTML structure.\n",
    "          #  plain_text: A list containing plain text representations of each paragraph (<p>) or list (<ol> or <ul>) present in the simplified Readability.js article HTML in the content field. Each paragraph or list is represented as a single string. List strings look like \"* item 1, * item 2, * item 3,\" for both ordered and unordered lists (note the trailing ,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(b'', None)"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "bashCommand = \"git clone https://github.com/alan-turing-institute/ReadabiliPy.git\"\n",
    "import subprocess\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* add multilabel classification with  sklearn.multiclass.OneVsRestClassifier to learn proximity beween questions\n",
    "* FIX BIG BUG IN models.py for question-object couples that don't already have a score.\n",
    "* adjust weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk\n",
    "#pip install any packages you don't have\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re, spacy, gensim\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/df_with_gensim_summaries.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0', 'Unnamed: 0.1.1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.first_100 = df.first_100.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized_first_100'] = df.first_100.apply(lambda x: word_tokenize(x, language = 'en'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(set(stopwords.words('english'))) + list(punctuation) + ['s', \"'\", 't', 'and', '\"', 'a', 'or', '/', 'in',\n",
    "                                                                    'for', '&', '-', \"''\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove stop words\n",
    "def remove_stops(text):\n",
    "    text_no_stops = []\n",
    "    for i in text:\n",
    "        if i not in stops:\n",
    "            if len(i) == 1:\n",
    "                pass\n",
    "            else:\n",
    "                text_no_stops.append(i)\n",
    "        else:\n",
    "            pass\n",
    "    return text_no_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['first_100_no_stops'] = df['tokenized_first_100'].apply(lambda x: remove_stops(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that it worked\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize WordNetLemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    lemmatized = []\n",
    "    for word in text:\n",
    "        lemmatized.append(lemmatizer.lemmatize(word))\n",
    "    return lemmatized\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatize_first_100'] = df['first_100_no_stops'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatize_first_100'] = df['lemmatize_first_100'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checkpoint -- save to csv\n",
    "#df.to_csv('df_with_lemmings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>KMEANS CLUSTERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Detect languages of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['language'] = df['lemmatize_first_100'].apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>gensim_summary</th>\n",
       "      <th>first_100</th>\n",
       "      <th>sent_tokenized</th>\n",
       "      <th>tokenized_first_100</th>\n",
       "      <th>first_100_no_stops</th>\n",
       "      <th>lemmatize_first_100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>en</th>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>84941</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "      <td>97038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>es</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fr</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>it</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ko</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          title  content  category  gensim_summary  first_100  sent_tokenized  \\\n",
       "language                                                                        \n",
       "en        97038    97038     84941           97038      97038           97038   \n",
       "es            4        4         2               4          4               4   \n",
       "fr           19       19        19              19         19              19   \n",
       "it            2        2         2               2          2               2   \n",
       "ko            1        1         1               1          1               1   \n",
       "\n",
       "          tokenized_first_100  first_100_no_stops  lemmatize_first_100  \n",
       "language                                                                \n",
       "en                      97038               97038                97038  \n",
       "es                          4                   4                    4  \n",
       "fr                         19                  19                   19  \n",
       "it                          2                   2                    2  \n",
       "ko                          1                   1                    1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('language').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows that are not english\n",
    "df = df.loc[df['language'] == 'en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv('df_english_articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Data/df_english_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to stem each word in a list and concat the list\n",
    "def stem_list(lst):\n",
    "    stemmed_list = []\n",
    "    for i in lst:\n",
    "        stemmed_list.append(stemmer.stem(i))\n",
    "    stem_string = ' '.join(stemmed_list)\n",
    "    return stem_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list contained in string to a regular list so it can be stemmed\n",
    "df['stemmed'] = df[\"first_100_no_stops\"].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stem words in list\n",
    "df['stemmed'] = df[\"stemmed\"].apply(lambda x: stem_list(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify that it worked\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop specific duplicate rows\n",
    "df = df[~df['stemmed'].str.contains(\"archiveteam.org contain\", case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95790, 12)"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT --- SAVE TO CSV\n",
    "#df.to_csv('df_with_stems_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT --- RUN TO OPEN CSV IF STARTING WORK HERE\n",
    "#df = pd.read_csv('df_with_stems_final.csv')\n",
    "#df.drop(['Unnamed: 0', 'Unnamed: 0.1'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of stemmed document strings\n",
    "documents = df['stemmed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preview list\n",
    "#documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to vectorize strings and perform tf-idf transformation\n",
    "def vectorize_texts(list_of_strings):\n",
    "    print('Performing vectorization and TF/IDF transformation on texts...')\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(list_of_strings)\n",
    "    transformer = TfidfTransformer(smooth_idf=False)\n",
    "    tfidf = transformer.fit_transform(X)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_texts(num_clusters, tfidf):\n",
    "    #perform kmeans clustering for range of clusters\n",
    "    print('Beginning KMeans Clustering, number of clusters = ', num_clusters, '\\n') \n",
    "    km = KMeans(n_clusters=num_clusters, max_iter = 100, verbose = 2, n_init = 1).fit(tfidf)\n",
    "    \n",
    "    \n",
    "    return km"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Run Clustering for range of K's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing vectorization and TF/IDF transformation on texts...\n"
     ]
    }
   ],
   "source": [
    "#vectorized the list of stemmed documents\n",
    "documents_vectorized = vectorize_texts(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans3 = cluster_texts(3, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans4= cluster_texts(4, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans5= cluster_texts(5, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans6= cluster_texts(6, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans7= cluster_texts(7, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans8= cluster_texts(8, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans9= cluster_texts(9, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans10= cluster_texts(10, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans11= cluster_texts(11, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans12= cluster_texts(12, documents_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save kmeans12 model for further use\n",
    "pickle.dump(kmeans12, open(\"../Models/kmeans12.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load back in kmeans12 model\n",
    "#kmeans = pickle.load(open(\"kmeans12.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df.columns = ['kmeans3', 'kmeans4', 'kmeans5', 'kmeans6', 'kmeans7', 'kmeans8', 'kmeans9', 'kmeans10', 'kmeans11', 'kmeans12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df['kmeans12'] = kmeans12.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df['stemmed'] = df['stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(95790, 11)\n",
      "(95790, 11)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(kmeans_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kmeans_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHECKPOINT --- SAVE TO CSV TO AVOID RUNNING KMEANS FUNCTIONS AGAIN\n",
    "kmeans_df.to_csv('../Models/kmeans_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_df = pd.read_csv('../Modelskmeans_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Check Clusters for K's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    55197\n",
       "1    20409\n",
       "2    20184\n",
       "Name: kmeans3, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_df['kmeans3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3     38169\n",
       "0     18915\n",
       "1      9671\n",
       "10     6120\n",
       "11     6033\n",
       "2      4171\n",
       "4      3634\n",
       "6      2185\n",
       "5      2171\n",
       "8      1883\n",
       "7      1869\n",
       "9       969\n",
       "Name: kmeans12, dtype: int64"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_df['kmeans12'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, '0'),\n",
       " Text(0, 0, '1'),\n",
       " Text(0, 0, '2'),\n",
       " Text(0, 0, '3'),\n",
       " Text(0, 0, '4'),\n",
       " Text(0, 0, '5'),\n",
       " Text(0, 0, '6'),\n",
       " Text(0, 0, '7'),\n",
       " Text(0, 0, '8'),\n",
       " Text(0, 0, '9'),\n",
       " Text(0, 0, '10'),\n",
       " Text(0, 0, '11')]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.countplot(x= 'kmeans12', data=kmeans_df)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the most common words within each cluster\n",
    "def get_most_common_words(df, df_column, num_words):\n",
    "    common_words = []\n",
    "    for i in range(0,12):\n",
    "        common = Counter(\" \".join(df.loc[df_column == i]['stemmed']).split()).most_common(num_words)\n",
    "        for j in common:\n",
    "            dict_ = {}\n",
    "            dict_['cluster'] = i\n",
    "            dict_['word'] = j[0]\n",
    "            common_words.append(dict_)\n",
    "            \n",
    "    return common_words    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster': 0, 'word': 'state'},\n",
       " {'cluster': 0, 'word': 'said'},\n",
       " {'cluster': 0, 'word': \"'s\"},\n",
       " {'cluster': 0, 'word': 'new'},\n",
       " {'cluster': 0, 'word': 'year'},\n",
       " {'cluster': 0, 'word': 'presid'},\n",
       " {'cluster': 0, 'word': 'peopl'},\n",
       " {'cluster': 0, 'word': 'nation'},\n",
       " {'cluster': 0, 'word': '``'},\n",
       " {'cluster': 0, 'word': 'one'},\n",
       " {'cluster': 0, 'word': 'unit'},\n",
       " {'cluster': 0, 'word': 'countri'},\n",
       " {'cluster': 0, 'word': 'govern'},\n",
       " {'cluster': 0, 'word': 'would'},\n",
       " {'cluster': 0, 'word': 'report'},\n",
       " {'cluster': 0, 'word': 'two'},\n",
       " {'cluster': 0, 'word': 'american'},\n",
       " {'cluster': 0, 'word': 'last'},\n",
       " {'cluster': 0, 'word': 'u.s.'},\n",
       " {'cluster': 0, 'word': 'offici'},\n",
       " {'cluster': 0, 'word': 'week'},\n",
       " {'cluster': 0, 'word': 'citi'},\n",
       " {'cluster': 0, 'word': 'time'},\n",
       " {'cluster': 0, 'word': 'polit'},\n",
       " {'cluster': 0, 'word': 'day'},\n",
       " {'cluster': 1, 'word': 'trump'},\n",
       " {'cluster': 1, 'word': 'presid'},\n",
       " {'cluster': 1, 'word': 'donald'},\n",
       " {'cluster': 1, 'word': 'said'},\n",
       " {'cluster': 1, 'word': \"'s\"},\n",
       " {'cluster': 1, 'word': 'mr.'},\n",
       " {'cluster': 1, 'word': '``'},\n",
       " {'cluster': 1, 'word': 'would'},\n",
       " {'cluster': 1, 'word': 'new'},\n",
       " {'cluster': 1, 'word': 'white'},\n",
       " {'cluster': 1, 'word': 'state'},\n",
       " {'cluster': 1, 'word': 'hous'},\n",
       " {'cluster': 1, 'word': 'one'},\n",
       " {'cluster': 1, 'word': 'campaign'},\n",
       " {'cluster': 1, 'word': 'first'},\n",
       " {'cluster': 1, 'word': 'american'},\n",
       " {'cluster': 1, 'word': 'washington'},\n",
       " {'cluster': 1, 'word': 'administr'},\n",
       " {'cluster': 1, 'word': 'nation'},\n",
       " {'cluster': 1, 'word': 'unit'},\n",
       " {'cluster': 1, 'word': 'week'},\n",
       " {'cluster': 1, 'word': 'u.s.'},\n",
       " {'cluster': 1, 'word': 'day'},\n",
       " {'cluster': 1, 'word': 'obama'},\n",
       " {'cluster': 1, 'word': 'peopl'},\n",
       " {'cluster': 2, 'word': 'trump'},\n",
       " {'cluster': 2, 'word': 'republican'},\n",
       " {'cluster': 2, 'word': 'parti'},\n",
       " {'cluster': 2, 'word': 'donald'},\n",
       " {'cluster': 2, 'word': 'democrat'},\n",
       " {'cluster': 2, 'word': 'presidenti'},\n",
       " {'cluster': 2, 'word': 'senat'},\n",
       " {'cluster': 2, 'word': \"'s\"},\n",
       " {'cluster': 2, 'word': 'elect'},\n",
       " {'cluster': 2, 'word': 'candid'},\n",
       " {'cluster': 2, 'word': 'cruz'},\n",
       " {'cluster': 2, 'word': 'vote'},\n",
       " {'cluster': 2, 'word': 'state'},\n",
       " {'cluster': 2, 'word': 'campaign'},\n",
       " {'cluster': 2, 'word': 'presid'},\n",
       " {'cluster': 2, 'word': 'polit'},\n",
       " {'cluster': 2, 'word': 'new'},\n",
       " {'cluster': 2, 'word': 'gop'},\n",
       " {'cluster': 2, 'word': 'one'},\n",
       " {'cluster': 2, 'word': 'said'},\n",
       " {'cluster': 2, 'word': 'hous'},\n",
       " {'cluster': 2, 'word': 'voter'},\n",
       " {'cluster': 2, 'word': 'nomine'},\n",
       " {'cluster': 2, 'word': '``'},\n",
       " {'cluster': 2, 'word': 'nation'},\n",
       " {'cluster': 3, 'word': 'one'},\n",
       " {'cluster': 3, 'word': 'year'},\n",
       " {'cluster': 3, 'word': 'new'},\n",
       " {'cluster': 3, 'word': '``'},\n",
       " {'cluster': 3, 'word': \"'s\"},\n",
       " {'cluster': 3, 'word': 'like'},\n",
       " {'cluster': 3, 'word': 'time'},\n",
       " {'cluster': 3, 'word': 'first'},\n",
       " {'cluster': 3, 'word': 'said'},\n",
       " {'cluster': 3, 'word': 'two'},\n",
       " {'cluster': 3, 'word': 'peopl'},\n",
       " {'cluster': 3, 'word': 'day'},\n",
       " {'cluster': 3, 'word': 'last'},\n",
       " {'cluster': 3, 'word': 'would'},\n",
       " {'cluster': 3, 'word': 'get'},\n",
       " {'cluster': 3, 'word': 'say'},\n",
       " {'cluster': 3, 'word': 'make'},\n",
       " {'cluster': 3, 'word': 'work'},\n",
       " {'cluster': 3, 'word': 'even'},\n",
       " {'cluster': 3, 'word': 'show'},\n",
       " {'cluster': 3, 'word': 'world'},\n",
       " {'cluster': 3, 'word': 'go'},\n",
       " {'cluster': 3, 'word': 'game'},\n",
       " {'cluster': 3, 'word': 'live'},\n",
       " {'cluster': 3, 'word': 'week'},\n",
       " {'cluster': 4, 'word': 'clinton'},\n",
       " {'cluster': 4, 'word': 'hillari'},\n",
       " {'cluster': 4, 'word': 'trump'},\n",
       " {'cluster': 4, 'word': 'democrat'},\n",
       " {'cluster': 4, 'word': 'campaign'},\n",
       " {'cluster': 4, 'word': 'donald'},\n",
       " {'cluster': 4, 'word': 'presidenti'},\n",
       " {'cluster': 4, 'word': 'state'},\n",
       " {'cluster': 4, 'word': \"'s\"},\n",
       " {'cluster': 4, 'word': 'sander'},\n",
       " {'cluster': 4, 'word': 'presid'},\n",
       " {'cluster': 4, 'word': 'elect'},\n",
       " {'cluster': 4, 'word': 'said'},\n",
       " {'cluster': 4, 'word': 'new'},\n",
       " {'cluster': 4, 'word': 'email'},\n",
       " {'cluster': 4, 'word': '``'},\n",
       " {'cluster': 4, 'word': 'republican'},\n",
       " {'cluster': 4, 'word': 'poll'},\n",
       " {'cluster': 4, 'word': 'would'},\n",
       " {'cluster': 4, 'word': 'candid'},\n",
       " {'cluster': 4, 'word': 'one'},\n",
       " {'cluster': 4, 'word': 'berni'},\n",
       " {'cluster': 4, 'word': 'vote'},\n",
       " {'cluster': 4, 'word': 'polit'},\n",
       " {'cluster': 4, 'word': 'support'},\n",
       " {'cluster': 5, 'word': 'trump'},\n",
       " {'cluster': 5, 'word': 'presid'},\n",
       " {'cluster': 5, 'word': 'russian'},\n",
       " {'cluster': 5, 'word': 'russia'},\n",
       " {'cluster': 5, 'word': 'investig'},\n",
       " {'cluster': 5, 'word': 'intellig'},\n",
       " {'cluster': 5, 'word': 'comey'},\n",
       " {'cluster': 5, 'word': 'elect'},\n",
       " {'cluster': 5, 'word': 'said'},\n",
       " {'cluster': 5, 'word': 'mr.'},\n",
       " {'cluster': 5, 'word': 'offici'},\n",
       " {'cluster': 5, 'word': 'hous'},\n",
       " {'cluster': 5, 'word': \"'s\"},\n",
       " {'cluster': 5, 'word': 'director'},\n",
       " {'cluster': 5, 'word': 'fbi'},\n",
       " {'cluster': 5, 'word': 'donald'},\n",
       " {'cluster': 5, 'word': 'campaign'},\n",
       " {'cluster': 5, 'word': 'nation'},\n",
       " {'cluster': 5, 'word': 'former'},\n",
       " {'cluster': 5, 'word': 'committe'},\n",
       " {'cluster': 5, 'word': 'washington'},\n",
       " {'cluster': 5, 'word': 'u.s.'},\n",
       " {'cluster': 5, 'word': 'putin'},\n",
       " {'cluster': 5, 'word': 'jame'},\n",
       " {'cluster': 5, 'word': 'flynn'},\n",
       " {'cluster': 6, 'word': 'school'},\n",
       " {'cluster': 6, 'word': 'student'},\n",
       " {'cluster': 6, 'word': 'univers'},\n",
       " {'cluster': 6, 'word': 'educ'},\n",
       " {'cluster': 6, 'word': 'colleg'},\n",
       " {'cluster': 6, 'word': 'year'},\n",
       " {'cluster': 6, 'word': 'high'},\n",
       " {'cluster': 6, 'word': 'new'},\n",
       " {'cluster': 6, 'word': 'said'},\n",
       " {'cluster': 6, 'word': 'one'},\n",
       " {'cluster': 6, 'word': \"'s\"},\n",
       " {'cluster': 6, 'word': 'state'},\n",
       " {'cluster': 6, 'word': '``'},\n",
       " {'cluster': 6, 'word': 'teacher'},\n",
       " {'cluster': 6, 'word': 'public'},\n",
       " {'cluster': 6, 'word': 'last'},\n",
       " {'cluster': 6, 'word': 'would'},\n",
       " {'cluster': 6, 'word': 'first'},\n",
       " {'cluster': 6, 'word': 'time'},\n",
       " {'cluster': 6, 'word': 'two'},\n",
       " {'cluster': 6, 'word': 'day'},\n",
       " {'cluster': 6, 'word': 'week'},\n",
       " {'cluster': 6, 'word': 'class'},\n",
       " {'cluster': 6, 'word': 'graduat'},\n",
       " {'cluster': 6, 'word': 'peopl'},\n",
       " {'cluster': 7, 'word': 'court'},\n",
       " {'cluster': 7, 'word': 'suprem'},\n",
       " {'cluster': 7, 'word': 'justic'},\n",
       " {'cluster': 7, 'word': 'presid'},\n",
       " {'cluster': 7, 'word': 'judg'},\n",
       " {'cluster': 7, 'word': 'trump'},\n",
       " {'cluster': 7, 'word': \"'s\"},\n",
       " {'cluster': 7, 'word': 'rule'},\n",
       " {'cluster': 7, 'word': 'state'},\n",
       " {'cluster': 7, 'word': 'feder'},\n",
       " {'cluster': 7, 'word': 'senat'},\n",
       " {'cluster': 7, 'word': 'case'},\n",
       " {'cluster': 7, 'word': 'said'},\n",
       " {'cluster': 7, 'word': 'u.s.'},\n",
       " {'cluster': 7, 'word': 'law'},\n",
       " {'cluster': 7, 'word': 'appeal'},\n",
       " {'cluster': 7, 'word': 'gorsuch'},\n",
       " {'cluster': 7, 'word': 'would'},\n",
       " {'cluster': 7, 'word': 'order'},\n",
       " {'cluster': 7, 'word': 'decis'},\n",
       " {'cluster': 7, 'word': 'republican'},\n",
       " {'cluster': 7, 'word': 'obama'},\n",
       " {'cluster': 7, 'word': 'scalia'},\n",
       " {'cluster': 7, 'word': 'legal'},\n",
       " {'cluster': 7, 'word': 'one'},\n",
       " {'cluster': 8, 'word': 'republican'},\n",
       " {'cluster': 8, 'word': 'care'},\n",
       " {'cluster': 8, 'word': 'health'},\n",
       " {'cluster': 8, 'word': 'hous'},\n",
       " {'cluster': 8, 'word': 'bill'},\n",
       " {'cluster': 8, 'word': 'trump'},\n",
       " {'cluster': 8, 'word': 'act'},\n",
       " {'cluster': 8, 'word': 'senat'},\n",
       " {'cluster': 8, 'word': 'obamacar'},\n",
       " {'cluster': 8, 'word': 'presid'},\n",
       " {'cluster': 8, 'word': 'insur'},\n",
       " {'cluster': 8, 'word': 'would'},\n",
       " {'cluster': 8, 'word': 'repeal'},\n",
       " {'cluster': 8, 'word': 'afford'},\n",
       " {'cluster': 8, 'word': 'plan'},\n",
       " {'cluster': 8, 'word': 'tax'},\n",
       " {'cluster': 8, 'word': 'vote'},\n",
       " {'cluster': 8, 'word': 'american'},\n",
       " {'cluster': 8, 'word': 'law'},\n",
       " {'cluster': 8, 'word': 'legisl'},\n",
       " {'cluster': 8, 'word': 'replac'},\n",
       " {'cluster': 8, 'word': 'said'},\n",
       " {'cluster': 8, 'word': \"'s\"},\n",
       " {'cluster': 8, 'word': 'peopl'},\n",
       " {'cluster': 8, 'word': 'year'},\n",
       " {'cluster': 9, 'word': 'pleas'},\n",
       " {'cluster': 9, 'word': 'ad'},\n",
       " {'cluster': 9, 'word': 'stori'},\n",
       " {'cluster': 9, 'word': 'follow'},\n",
       " {'cluster': 9, 'word': 'great'},\n",
       " {'cluster': 9, 'word': 'need'},\n",
       " {'cluster': 9, 'word': 'write'},\n",
       " {'cluster': 9, 'word': 'continu'},\n",
       " {'cluster': 9, 'word': 'step'},\n",
       " {'cluster': 9, 'word': 'block'},\n",
       " {'cluster': 9, 'word': 'us'},\n",
       " {'cluster': 9, 'word': 'display'},\n",
       " {'cluster': 9, 'word': 'select'},\n",
       " {'cluster': 9, 'word': 'extens'},\n",
       " {'cluster': 9, 'word': 'email'},\n",
       " {'cluster': 9, 'word': 'trump'},\n",
       " {'cluster': 9, 'word': 'daili'},\n",
       " {'cluster': 9, 'word': 'also'},\n",
       " {'cluster': 9, 'word': 'atlant'},\n",
       " {'cluster': 9, 'word': 'part'},\n",
       " {'cluster': 9, 'word': 'idea'},\n",
       " {'cluster': 9, 'word': 'enter'},\n",
       " {'cluster': 9, 'word': 'special'},\n",
       " {'cluster': 9, 'word': 'sign'},\n",
       " {'cluster': 9, 'word': 'address'},\n",
       " {'cluster': 10, 'word': 'compani'},\n",
       " {'cluster': 10, 'word': \"'s\"},\n",
       " {'cluster': 10, 'word': 'percent'},\n",
       " {'cluster': 10, 'word': 'said'},\n",
       " {'cluster': 10, 'word': 'year'},\n",
       " {'cluster': 10, 'word': 'u.s.'},\n",
       " {'cluster': 10, 'word': 'market'},\n",
       " {'cluster': 10, 'word': '``'},\n",
       " {'cluster': 10, 'word': 'billion'},\n",
       " {'cluster': 10, 'word': 'bank'},\n",
       " {'cluster': 10, 'word': 'new'},\n",
       " {'cluster': 10, 'word': 'would'},\n",
       " {'cluster': 10, 'word': 'price'},\n",
       " {'cluster': 10, 'word': 'rate'},\n",
       " {'cluster': 10, 'word': 'last'},\n",
       " {'cluster': 10, 'word': 'stock'},\n",
       " {'cluster': 10, 'word': 'investor'},\n",
       " {'cluster': 10, 'word': 'share'},\n",
       " {'cluster': 10, 'word': 'one'},\n",
       " {'cluster': 10, 'word': 'busi'},\n",
       " {'cluster': 10, 'word': 'week'},\n",
       " {'cluster': 10, 'word': 'report'},\n",
       " {'cluster': 10, 'word': 'million'},\n",
       " {'cluster': 10, 'word': 'month'},\n",
       " {'cluster': 10, 'word': 'oil'},\n",
       " {'cluster': 11, 'word': 'polic'},\n",
       " {'cluster': 11, 'word': 'said'},\n",
       " {'cluster': 11, 'word': 'state'},\n",
       " {'cluster': 11, 'word': 'attack'},\n",
       " {'cluster': 11, 'word': 'offic'},\n",
       " {'cluster': 11, 'word': 'kill'},\n",
       " {'cluster': 11, 'word': \"'s\"},\n",
       " {'cluster': 11, 'word': 'north'},\n",
       " {'cluster': 11, 'word': 'citi'},\n",
       " {'cluster': 11, 'word': 'peopl'},\n",
       " {'cluster': 11, 'word': 'korea'},\n",
       " {'cluster': 11, 'word': 'islam'},\n",
       " {'cluster': 11, 'word': 'two'},\n",
       " {'cluster': 11, 'word': 'forc'},\n",
       " {'cluster': 11, 'word': '``'},\n",
       " {'cluster': 11, 'word': 'syrian'},\n",
       " {'cluster': 11, 'word': 'syria'},\n",
       " {'cluster': 11, 'word': 'offici'},\n",
       " {'cluster': 11, 'word': 'one'},\n",
       " {'cluster': 11, 'word': 'presid'},\n",
       " {'cluster': 11, 'word': 'unit'},\n",
       " {'cluster': 11, 'word': 'year'},\n",
       " {'cluster': 11, 'word': 'u.s.'},\n",
       " {'cluster': 11, 'word': 'man'},\n",
       " {'cluster': 11, 'word': 'militari'}]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_common_words(kmeans_df, kmeans_df['kmeans12'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('polic', 5375),\n",
       " ('offic', 2748),\n",
       " ('said', 2244),\n",
       " ('kill', 1398),\n",
       " ('man', 1101),\n",
       " ('shoot', 975),\n",
       " ('peopl', 967),\n",
       " (\"'s\", 935),\n",
       " ('shot', 927),\n",
       " ('attack', 915)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(\" \".join(kmeans_df.loc[kmeans_df['kmeans12'] == 9]['stemmed']).split()).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check clusters manually\n",
    "#kmeans_df.loc[kmeans_df['kmeans12'] == 9]['stemmed'][:10].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_2 = [\"\"\"He has been training populists in Italy and fomenting revolution in Belgium. He has been down in Texas,\n",
    "building a wall. But these have all been halting projects, placeholders. The real question is whether Steve Bannon \n",
    "will be back by Donald Trump’s side in 2020, after a two-year exile from Trumpworld. That could happen, at least \n",
    "according to Trump himself. “I’ll tell you one thing,” Trump said when I asked him about Bannon in February, \n",
    "during an Oval Office interview. “I watched Bannon a few times, four or five times over the last six months. \n",
    "Nobody says anything better about me right now than Bannon. I don’t know.” Bannon, of course, was the mastermind who \n",
    "took over a faltering Trump campaign in August 2016, guiding it to improbable victory. He then served as Trump’s \n",
    "chief political strategist in the White House, only to be forced out by Trump’s second chief of staff, John Kelly, \n",
    "in a move that a plainly exhausted Bannon seemed to almost welcome after months of battling the president’s \n",
    "son-in-law, Jared Kushner, the president’s daughter Ivanka Trump and, well, pretty much everyone else.\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_prep(sample_text):\n",
    "    tokens = word_tokenize(str(sample_text), language = 'en')\n",
    "    no_stops = remove_stops(tokens)\n",
    "    stemmed = stem_list(no_stops)\n",
    "    stemmed = stemmed.replace('\\\\n', '')\n",
    "    return [stemmed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'he train populist itali foment revolut belgium He texa build wall but halt project placehold the real question whether steve bannon will back donald trump side 2020 two-year exil trumpworld that could happen least accord trump tell one thing trump said ask bannon februari dure oval offic interview watch bannon time four five time last six month nobodi say anyth better right bannon know. bannon cours mastermind took falter trump campaign august 2016 guid improb victori He serv trump chief polit strategist white hous forc trump second chief staff john kelli in move plainli exhaust bannon seem almost welcom month battl presid son-in-law jare kushner presid daughter ivanka trump well pretti much everyon els\"]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_prep(X_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Merge Kmeans12 into original df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters = pd.concat([df, kmeans_df['kmeans12']], axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clusters.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overwrite previous csv with clusters df\n",
    "df_clusters.to_csv('../Data/df_english_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95790, 12)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clusters.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Re-Cluster Cluster #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans12_cluster3 = pd.DataFrame(df_clusters['kmeans12'].loc[df_clusters['kmeans12'] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans12_cluster3['stemmed'] = kmeans_df['stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kmeans12</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>never showtim new seri reviv spoiler ahead epi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>alphago victori defeat human opportun loss hum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>weapon war becam weapon web everi year artist ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>insid test flight facebook first internet dron...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>gadget shop chanc soon plunk cash new smartpho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>rare sunni day seattl phil spencer seem pleas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3</td>\n",
       "      <td>termin resurrectedon set arnold could best ter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>start shot jameson 9:45pm la vega feel like pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>crisp afternoon late last year made way manhat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>reach break point mani parent two half month n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>airport trigger anxieti subway system caus par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3</td>\n",
       "      <td>appl final announc augment realiti platform de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>internet ad-track machin true long enough rare...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3</td>\n",
       "      <td>wonder woman begin paradisiac island home amaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3</td>\n",
       "      <td>facial recognit system made huge technolog lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>scientist move mous whisker ear paw use electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3</td>\n",
       "      <td>steven univers emerg one import cartoon televi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3</td>\n",
       "      <td>imagin futur tap app smartphon summon self-dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>technolog startup fleet prone failur crop disa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>3</td>\n",
       "      <td>trick brain feel pain without medic use electr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>put monkey diet delay health problem old age n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>audienc associ word horror scari movi terrifi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>thing vine becom internet premier tool make sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3</td>\n",
       "      <td>person desktop comput use exclus expens machin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>'s brew mac vs. pc war back appl microsoft sha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>3</td>\n",
       "      <td>today big tech news might fancy-look surfac st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>3</td>\n",
       "      <td>public event like convent strap virtual realit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3</td>\n",
       "      <td>explod phone seem like freak accid chemic prop...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>3</td>\n",
       "      <td>nathan copeland abl move leg hand sinc broke n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>3</td>\n",
       "      <td>motion sick long bane virtual realiti associ s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95746</th>\n",
       "      <td>3</td>\n",
       "      <td>anyon ever watch westminst kennel club dog sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95748</th>\n",
       "      <td>3</td>\n",
       "      <td>nearli dozen year ago jo blackwell-preston kam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95749</th>\n",
       "      <td>3</td>\n",
       "      <td>winter olymp fashion larg tale two event open ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95750</th>\n",
       "      <td>3</td>\n",
       "      <td>runtuna sweden cri hell hell ring across snow-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95751</th>\n",
       "      <td>3</td>\n",
       "      <td>citi pizza-craz new york pizza war erupt regul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95752</th>\n",
       "      <td>3</td>\n",
       "      <td>record-set 5,694th home run 2017 season soar r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95753</th>\n",
       "      <td>3</td>\n",
       "      <td>kennedi space center fla. search cosmic real e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95755</th>\n",
       "      <td>3</td>\n",
       "      <td>ann jeffrey sophist blond actress singer play ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95756</th>\n",
       "      <td>3</td>\n",
       "      <td>mother singl parent much youth work day 12 yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95760</th>\n",
       "      <td>3</td>\n",
       "      <td>live affluent commun new jersey scientist ph.d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95761</th>\n",
       "      <td>3</td>\n",
       "      <td>blink light click sound coin perk like free in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95762</th>\n",
       "      <td>3</td>\n",
       "      <td>bill henfey park three block beach new jersey ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95763</th>\n",
       "      <td>3</td>\n",
       "      <td>kim velsey lifelong runner suffer hamstr injur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95764</th>\n",
       "      <td>3</td>\n",
       "      <td>long oddli far-flung collabor colorado museum ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95765</th>\n",
       "      <td>3</td>\n",
       "      <td>jame alex field jr. ohio charg second-degre mu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95766</th>\n",
       "      <td>3</td>\n",
       "      <td>basebal great power partnership rang foundat b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95767</th>\n",
       "      <td>3</td>\n",
       "      <td>star stripe project beaux-art facad new york p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95769</th>\n",
       "      <td>3</td>\n",
       "      <td>good morn olymp embarrass loyalti test black p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95772</th>\n",
       "      <td>3</td>\n",
       "      <td>olymp ice danc competit came dress monday nigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95773</th>\n",
       "      <td>3</td>\n",
       "      <td>got water saturday lewi pugh terrifi mr. pugh ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95776</th>\n",
       "      <td>3</td>\n",
       "      <td>atlanta profession athlet hello roger feder tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95779</th>\n",
       "      <td>3</td>\n",
       "      <td>renov apart stone hous croatia 2.1 million 1.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95780</th>\n",
       "      <td>3</td>\n",
       "      <td>adam wilk met left-hand pitch major leagu two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95781</th>\n",
       "      <td>3</td>\n",
       "      <td>follow text statement releas monday beverli yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95783</th>\n",
       "      <td>3</td>\n",
       "      <td>rick roja report time grew beaumont beaumont t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95784</th>\n",
       "      <td>3</td>\n",
       "      <td>london might call triumphal return angel ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95786</th>\n",
       "      <td>3</td>\n",
       "      <td>band popular song night drove old dixi ex-conf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95787</th>\n",
       "      <td>3</td>\n",
       "      <td>rev billi graham admit later year learn hard l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95788</th>\n",
       "      <td>3</td>\n",
       "      <td>sammi stewart set record struck seven batter r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95789</th>\n",
       "      <td>3</td>\n",
       "      <td>good morn want get california today email sign...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38169 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       kmeans12                                            stemmed\n",
       "0             3  never showtim new seri reviv spoiler ahead epi...\n",
       "1             3  alphago victori defeat human opportun loss hum...\n",
       "2             3  weapon war becam weapon web everi year artist ...\n",
       "4             3  insid test flight facebook first internet dron...\n",
       "5             3  gadget shop chanc soon plunk cash new smartpho...\n",
       "6             3  rare sunni day seattl phil spencer seem pleas ...\n",
       "7             3  termin resurrectedon set arnold could best ter...\n",
       "8             3  start shot jameson 9:45pm la vega feel like pl...\n",
       "9             3  crisp afternoon late last year made way manhat...\n",
       "10            3  reach break point mani parent two half month n...\n",
       "11            3  airport trigger anxieti subway system caus par...\n",
       "12            3  appl final announc augment realiti platform de...\n",
       "13            3  internet ad-track machin true long enough rare...\n",
       "14            3  wonder woman begin paradisiac island home amaz...\n",
       "15            3  facial recognit system made huge technolog lea...\n",
       "16            3  scientist move mous whisker ear paw use electr...\n",
       "18            3  steven univers emerg one import cartoon televi...\n",
       "20            3  imagin futur tap app smartphon summon self-dri...\n",
       "22            3  technolog startup fleet prone failur crop disa...\n",
       "26            3  trick brain feel pain without medic use electr...\n",
       "28            3  put monkey diet delay health problem old age n...\n",
       "29            3  audienc associ word horror scari movi terrifi ...\n",
       "30            3  thing vine becom internet premier tool make sh...\n",
       "31            3  person desktop comput use exclus expens machin...\n",
       "33            3  's brew mac vs. pc war back appl microsoft sha...\n",
       "34            3  today big tech news might fancy-look surfac st...\n",
       "35            3  public event like convent strap virtual realit...\n",
       "37            3  explod phone seem like freak accid chemic prop...\n",
       "38            3  nathan copeland abl move leg hand sinc broke n...\n",
       "39            3  motion sick long bane virtual realiti associ s...\n",
       "...         ...                                                ...\n",
       "95746         3  anyon ever watch westminst kennel club dog sho...\n",
       "95748         3  nearli dozen year ago jo blackwell-preston kam...\n",
       "95749         3  winter olymp fashion larg tale two event open ...\n",
       "95750         3  runtuna sweden cri hell hell ring across snow-...\n",
       "95751         3  citi pizza-craz new york pizza war erupt regul...\n",
       "95752         3  record-set 5,694th home run 2017 season soar r...\n",
       "95753         3  kennedi space center fla. search cosmic real e...\n",
       "95755         3  ann jeffrey sophist blond actress singer play ...\n",
       "95756         3  mother singl parent much youth work day 12 yea...\n",
       "95760         3  live affluent commun new jersey scientist ph.d...\n",
       "95761         3  blink light click sound coin perk like free in...\n",
       "95762         3  bill henfey park three block beach new jersey ...\n",
       "95763         3  kim velsey lifelong runner suffer hamstr injur...\n",
       "95764         3  long oddli far-flung collabor colorado museum ...\n",
       "95765         3  jame alex field jr. ohio charg second-degre mu...\n",
       "95766         3  basebal great power partnership rang foundat b...\n",
       "95767         3  star stripe project beaux-art facad new york p...\n",
       "95769         3  good morn olymp embarrass loyalti test black p...\n",
       "95772         3  olymp ice danc competit came dress monday nigh...\n",
       "95773         3  got water saturday lewi pugh terrifi mr. pugh ...\n",
       "95776         3  atlanta profession athlet hello roger feder tr...\n",
       "95779         3  renov apart stone hous croatia 2.1 million 1.7...\n",
       "95780         3  adam wilk met left-hand pitch major leagu two ...\n",
       "95781         3  follow text statement releas monday beverli yo...\n",
       "95783         3  rick roja report time grew beaumont beaumont t...\n",
       "95784         3  london might call triumphal return angel ameri...\n",
       "95786         3  band popular song night drove old dixi ex-conf...\n",
       "95787         3  rev billi graham admit later year learn hard l...\n",
       "95788         3  sammi stewart set record struck seven batter r...\n",
       "95789         3  good morn want get california today email sign...\n",
       "\n",
       "[38169 rows x 2 columns]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans12_cluster3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_cluster3 = kmeans12_cluster3['stemmed'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing vectorization and TF/IDF transformation on texts...\n"
     ]
    }
   ],
   "source": [
    "cluster_3_vectorized = vectorize_texts(documents_cluster3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning KMeans Clustering, number of clusters =  10 \n",
      "\n",
      "Initialization complete\n",
      "Iteration  0, inertia 73317.229\n",
      "Iteration  1, inertia 37568.404\n",
      "Iteration  2, inertia 37503.549\n",
      "Iteration  3, inertia 37481.140\n",
      "Iteration  4, inertia 37468.097\n",
      "Iteration  5, inertia 37459.530\n",
      "Iteration  6, inertia 37454.379\n",
      "Iteration  7, inertia 37451.283\n",
      "Iteration  8, inertia 37449.074\n",
      "Iteration  9, inertia 37447.200\n",
      "Iteration 10, inertia 37445.578\n",
      "Iteration 11, inertia 37444.275\n",
      "Iteration 12, inertia 37443.478\n",
      "Iteration 13, inertia 37442.821\n",
      "Iteration 14, inertia 37442.291\n",
      "Iteration 15, inertia 37441.787\n",
      "Iteration 16, inertia 37441.204\n",
      "Iteration 17, inertia 37440.715\n",
      "Iteration 18, inertia 37440.219\n",
      "Iteration 19, inertia 37439.785\n",
      "Iteration 20, inertia 37439.467\n",
      "Iteration 21, inertia 37439.173\n",
      "Iteration 22, inertia 37438.960\n",
      "Iteration 23, inertia 37438.762\n",
      "Iteration 24, inertia 37438.615\n",
      "Iteration 25, inertia 37438.483\n",
      "Iteration 26, inertia 37438.205\n",
      "Iteration 27, inertia 37437.744\n",
      "Iteration 28, inertia 37436.589\n",
      "Iteration 29, inertia 37434.555\n",
      "Iteration 30, inertia 37434.091\n",
      "Iteration 31, inertia 37434.006\n",
      "Iteration 32, inertia 37433.932\n",
      "Iteration 33, inertia 37433.896\n",
      "Iteration 34, inertia 37433.871\n",
      "Iteration 35, inertia 37433.851\n",
      "Iteration 36, inertia 37433.843\n",
      "Iteration 37, inertia 37433.837\n",
      "Iteration 38, inertia 37433.835\n",
      "Iteration 39, inertia 37433.831\n",
      "Iteration 40, inertia 37433.827\n",
      "Iteration 41, inertia 37433.824\n",
      "Iteration 42, inertia 37433.822\n",
      "Iteration 43, inertia 37433.822\n",
      "Iteration 44, inertia 37433.821\n",
      "Converged at iteration 44: center shift 0.000000e+00 within tolerance 1.443749e-09\n"
     ]
    }
   ],
   "source": [
    "kmeans10_cluster3= cluster_texts(10, cluster_3_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_10_cluster3 = pd.DataFrame(kmeans10_cluster3.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_10_cluster3['stemmed'] = kmeans_df['stemmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    9472\n",
       "5    5874\n",
       "6    4579\n",
       "9    4031\n",
       "3    3765\n",
       "8    3218\n",
       "7    2508\n",
       "2    2197\n",
       "1    1822\n",
       "0     703\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans_10_cluster3[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'cluster': 0, 'word': 'trump'},\n",
       " {'cluster': 0, 'word': '``'},\n",
       " {'cluster': 0, 'word': 'one'},\n",
       " {'cluster': 0, 'word': 'new'},\n",
       " {'cluster': 0, 'word': \"'s\"},\n",
       " {'cluster': 0, 'word': 'year'},\n",
       " {'cluster': 0, 'word': 'presid'},\n",
       " {'cluster': 0, 'word': 'said'},\n",
       " {'cluster': 0, 'word': 'donald'},\n",
       " {'cluster': 0, 'word': 'would'},\n",
       " {'cluster': 0, 'word': 'state'},\n",
       " {'cluster': 0, 'word': 'two'},\n",
       " {'cluster': 0, 'word': 'first'},\n",
       " {'cluster': 0, 'word': 'time'},\n",
       " {'cluster': 0, 'word': 'last'},\n",
       " {'cluster': 0, 'word': 'peopl'},\n",
       " {'cluster': 0, 'word': 'like'},\n",
       " {'cluster': 0, 'word': 'nation'},\n",
       " {'cluster': 0, 'word': 'day'},\n",
       " {'cluster': 0, 'word': 'say'},\n",
       " {'cluster': 0, 'word': 'week'},\n",
       " {'cluster': 0, 'word': 'clinton'},\n",
       " {'cluster': 0, 'word': 'american'},\n",
       " {'cluster': 0, 'word': 'make'},\n",
       " {'cluster': 0, 'word': 'republican'},\n",
       " {'cluster': 1, 'word': 'trump'},\n",
       " {'cluster': 1, 'word': '``'},\n",
       " {'cluster': 1, 'word': \"'s\"},\n",
       " {'cluster': 1, 'word': 'said'},\n",
       " {'cluster': 1, 'word': 'one'},\n",
       " {'cluster': 1, 'word': 'new'},\n",
       " {'cluster': 1, 'word': 'presid'},\n",
       " {'cluster': 1, 'word': 'year'},\n",
       " {'cluster': 1, 'word': 'state'},\n",
       " {'cluster': 1, 'word': 'donald'},\n",
       " {'cluster': 1, 'word': 'like'},\n",
       " {'cluster': 1, 'word': 'would'},\n",
       " {'cluster': 1, 'word': 'time'},\n",
       " {'cluster': 1, 'word': 'first'},\n",
       " {'cluster': 1, 'word': 'peopl'},\n",
       " {'cluster': 1, 'word': 'cnn'},\n",
       " {'cluster': 1, 'word': 'two'},\n",
       " {'cluster': 1, 'word': 'day'},\n",
       " {'cluster': 1, 'word': 'last'},\n",
       " {'cluster': 1, 'word': 'nation'},\n",
       " {'cluster': 1, 'word': 'week'},\n",
       " {'cluster': 1, 'word': 'republican'},\n",
       " {'cluster': 1, 'word': 'advertis'},\n",
       " {'cluster': 1, 'word': 'american'},\n",
       " {'cluster': 1, 'word': 'news'},\n",
       " {'cluster': 2, 'word': '``'},\n",
       " {'cluster': 2, 'word': 'trump'},\n",
       " {'cluster': 2, 'word': \"'s\"},\n",
       " {'cluster': 2, 'word': 'one'},\n",
       " {'cluster': 2, 'word': 'new'},\n",
       " {'cluster': 2, 'word': 'said'},\n",
       " {'cluster': 2, 'word': 'year'},\n",
       " {'cluster': 2, 'word': 'cnn'},\n",
       " {'cluster': 2, 'word': 'presid'},\n",
       " {'cluster': 2, 'word': 'state'},\n",
       " {'cluster': 2, 'word': 'peopl'},\n",
       " {'cluster': 2, 'word': 'like'},\n",
       " {'cluster': 2, 'word': 'donald'},\n",
       " {'cluster': 2, 'word': 'time'},\n",
       " {'cluster': 2, 'word': 'two'},\n",
       " {'cluster': 2, 'word': 'first'},\n",
       " {'cluster': 2, 'word': 'last'},\n",
       " {'cluster': 2, 'word': 'would'},\n",
       " {'cluster': 2, 'word': 'week'},\n",
       " {'cluster': 2, 'word': 'day'},\n",
       " {'cluster': 2, 'word': 'american'},\n",
       " {'cluster': 2, 'word': 'nation'},\n",
       " {'cluster': 2, 'word': 'republican'},\n",
       " {'cluster': 2, 'word': 'say'},\n",
       " {'cluster': 2, 'word': 'report'},\n",
       " {'cluster': 3, 'word': 'trump'},\n",
       " {'cluster': 3, 'word': '``'},\n",
       " {'cluster': 3, 'word': \"'s\"},\n",
       " {'cluster': 3, 'word': 'said'},\n",
       " {'cluster': 3, 'word': 'new'},\n",
       " {'cluster': 3, 'word': 'presid'},\n",
       " {'cluster': 3, 'word': 'one'},\n",
       " {'cluster': 3, 'word': 'year'},\n",
       " {'cluster': 3, 'word': 'state'},\n",
       " {'cluster': 3, 'word': 'donald'},\n",
       " {'cluster': 3, 'word': 'like'},\n",
       " {'cluster': 3, 'word': 'would'},\n",
       " {'cluster': 3, 'word': 'time'},\n",
       " {'cluster': 3, 'word': 'first'},\n",
       " {'cluster': 3, 'word': 'clinton'},\n",
       " {'cluster': 3, 'word': 'peopl'},\n",
       " {'cluster': 3, 'word': 'cnn'},\n",
       " {'cluster': 3, 'word': 'day'},\n",
       " {'cluster': 3, 'word': 'advertis'},\n",
       " {'cluster': 3, 'word': 'two'},\n",
       " {'cluster': 3, 'word': 'last'},\n",
       " {'cluster': 3, 'word': 'american'},\n",
       " {'cluster': 3, 'word': 'nation'},\n",
       " {'cluster': 3, 'word': 'week'},\n",
       " {'cluster': 3, 'word': 'mr.'},\n",
       " {'cluster': 4, 'word': 'trump'},\n",
       " {'cluster': 4, 'word': '``'},\n",
       " {'cluster': 4, 'word': 'said'},\n",
       " {'cluster': 4, 'word': 'one'},\n",
       " {'cluster': 4, 'word': 'new'},\n",
       " {'cluster': 4, 'word': 'presid'},\n",
       " {'cluster': 4, 'word': \"'s\"},\n",
       " {'cluster': 4, 'word': 'year'},\n",
       " {'cluster': 4, 'word': 'state'},\n",
       " {'cluster': 4, 'word': 'donald'},\n",
       " {'cluster': 4, 'word': 'time'},\n",
       " {'cluster': 4, 'word': 'would'},\n",
       " {'cluster': 4, 'word': 'peopl'},\n",
       " {'cluster': 4, 'word': 'first'},\n",
       " {'cluster': 4, 'word': 'like'},\n",
       " {'cluster': 4, 'word': 'two'},\n",
       " {'cluster': 4, 'word': 'day'},\n",
       " {'cluster': 4, 'word': 'american'},\n",
       " {'cluster': 4, 'word': 'last'},\n",
       " {'cluster': 4, 'word': 'week'},\n",
       " {'cluster': 4, 'word': 'cnn'},\n",
       " {'cluster': 4, 'word': 'nation'},\n",
       " {'cluster': 4, 'word': 'republican'},\n",
       " {'cluster': 4, 'word': 'clinton'},\n",
       " {'cluster': 4, 'word': 'say'},\n",
       " {'cluster': 5, 'word': 'trump'},\n",
       " {'cluster': 5, 'word': 'one'},\n",
       " {'cluster': 5, 'word': \"'s\"},\n",
       " {'cluster': 5, 'word': '``'},\n",
       " {'cluster': 5, 'word': 'said'},\n",
       " {'cluster': 5, 'word': 'new'},\n",
       " {'cluster': 5, 'word': 'year'},\n",
       " {'cluster': 5, 'word': 'presid'},\n",
       " {'cluster': 5, 'word': 'state'},\n",
       " {'cluster': 5, 'word': 'would'},\n",
       " {'cluster': 5, 'word': 'donald'},\n",
       " {'cluster': 5, 'word': 'peopl'},\n",
       " {'cluster': 5, 'word': 'time'},\n",
       " {'cluster': 5, 'word': 'first'},\n",
       " {'cluster': 5, 'word': 'like'},\n",
       " {'cluster': 5, 'word': 'two'},\n",
       " {'cluster': 5, 'word': 'last'},\n",
       " {'cluster': 5, 'word': 'republican'},\n",
       " {'cluster': 5, 'word': 'day'},\n",
       " {'cluster': 5, 'word': 'nation'},\n",
       " {'cluster': 5, 'word': 'american'},\n",
       " {'cluster': 5, 'word': 'cnn'},\n",
       " {'cluster': 5, 'word': 'week'},\n",
       " {'cluster': 5, 'word': 'clinton'},\n",
       " {'cluster': 5, 'word': 'say'},\n",
       " {'cluster': 6, 'word': 'trump'},\n",
       " {'cluster': 6, 'word': 'said'},\n",
       " {'cluster': 6, 'word': '``'},\n",
       " {'cluster': 6, 'word': 'presid'},\n",
       " {'cluster': 6, 'word': 'advertis'},\n",
       " {'cluster': 6, 'word': 'new'},\n",
       " {'cluster': 6, 'word': \"'s\"},\n",
       " {'cluster': 6, 'word': 'one'},\n",
       " {'cluster': 6, 'word': 'state'},\n",
       " {'cluster': 6, 'word': 'year'},\n",
       " {'cluster': 6, 'word': 'time'},\n",
       " {'cluster': 6, 'word': 'first'},\n",
       " {'cluster': 6, 'word': 'donald'},\n",
       " {'cluster': 6, 'word': 'would'},\n",
       " {'cluster': 6, 'word': 'peopl'},\n",
       " {'cluster': 6, 'word': 'last'},\n",
       " {'cluster': 6, 'word': 'two'},\n",
       " {'cluster': 6, 'word': 'news'},\n",
       " {'cluster': 6, 'word': 'nation'},\n",
       " {'cluster': 6, 'word': 'report'},\n",
       " {'cluster': 6, 'word': 'week'},\n",
       " {'cluster': 6, 'word': 'clinton'},\n",
       " {'cluster': 6, 'word': 'like'},\n",
       " {'cluster': 6, 'word': 'day'},\n",
       " {'cluster': 6, 'word': 'say'},\n",
       " {'cluster': 7, 'word': 'trump'},\n",
       " {'cluster': 7, 'word': 'one'},\n",
       " {'cluster': 7, 'word': 'presid'},\n",
       " {'cluster': 7, 'word': 'new'},\n",
       " {'cluster': 7, 'word': 'said'},\n",
       " {'cluster': 7, 'word': 'year'},\n",
       " {'cluster': 7, 'word': 'state'},\n",
       " {'cluster': 7, 'word': '``'},\n",
       " {'cluster': 7, 'word': \"'s\"},\n",
       " {'cluster': 7, 'word': 'donald'},\n",
       " {'cluster': 7, 'word': 'would'},\n",
       " {'cluster': 7, 'word': 'first'},\n",
       " {'cluster': 7, 'word': 'peopl'},\n",
       " {'cluster': 7, 'word': 'time'},\n",
       " {'cluster': 7, 'word': 'last'},\n",
       " {'cluster': 7, 'word': 'two'},\n",
       " {'cluster': 7, 'word': 'like'},\n",
       " {'cluster': 7, 'word': 'day'},\n",
       " {'cluster': 7, 'word': 'american'},\n",
       " {'cluster': 7, 'word': 'week'},\n",
       " {'cluster': 7, 'word': 'republican'},\n",
       " {'cluster': 7, 'word': 'advertis'},\n",
       " {'cluster': 7, 'word': 'clinton'},\n",
       " {'cluster': 7, 'word': 'nation'},\n",
       " {'cluster': 7, 'word': 'even'},\n",
       " {'cluster': 8, 'word': 'trump'},\n",
       " {'cluster': 8, 'word': '``'},\n",
       " {'cluster': 8, 'word': \"'s\"},\n",
       " {'cluster': 8, 'word': 'one'},\n",
       " {'cluster': 8, 'word': 'said'},\n",
       " {'cluster': 8, 'word': 'presid'},\n",
       " {'cluster': 8, 'word': 'new'},\n",
       " {'cluster': 8, 'word': 'year'},\n",
       " {'cluster': 8, 'word': 'state'},\n",
       " {'cluster': 8, 'word': 'donald'},\n",
       " {'cluster': 8, 'word': 'peopl'},\n",
       " {'cluster': 8, 'word': 'clinton'},\n",
       " {'cluster': 8, 'word': 'time'},\n",
       " {'cluster': 8, 'word': 'cnn'},\n",
       " {'cluster': 8, 'word': 'two'},\n",
       " {'cluster': 8, 'word': 'first'},\n",
       " {'cluster': 8, 'word': 'would'},\n",
       " {'cluster': 8, 'word': 'like'},\n",
       " {'cluster': 8, 'word': 'last'},\n",
       " {'cluster': 8, 'word': 'week'},\n",
       " {'cluster': 8, 'word': 'day'},\n",
       " {'cluster': 8, 'word': 'nation'},\n",
       " {'cluster': 8, 'word': 'advertis'},\n",
       " {'cluster': 8, 'word': 'american'},\n",
       " {'cluster': 8, 'word': 'republican'},\n",
       " {'cluster': 9, 'word': 'trump'},\n",
       " {'cluster': 9, 'word': 'said'},\n",
       " {'cluster': 9, 'word': '``'},\n",
       " {'cluster': 9, 'word': \"'s\"},\n",
       " {'cluster': 9, 'word': 'one'},\n",
       " {'cluster': 9, 'word': 'presid'},\n",
       " {'cluster': 9, 'word': 'new'},\n",
       " {'cluster': 9, 'word': 'year'},\n",
       " {'cluster': 9, 'word': 'state'},\n",
       " {'cluster': 9, 'word': 'time'},\n",
       " {'cluster': 9, 'word': 'cnn'},\n",
       " {'cluster': 9, 'word': 'donald'},\n",
       " {'cluster': 9, 'word': 'would'},\n",
       " {'cluster': 9, 'word': 'peopl'},\n",
       " {'cluster': 9, 'word': 'first'},\n",
       " {'cluster': 9, 'word': 'last'},\n",
       " {'cluster': 9, 'word': 'two'},\n",
       " {'cluster': 9, 'word': 'like'},\n",
       " {'cluster': 9, 'word': 'nation'},\n",
       " {'cluster': 9, 'word': 'day'},\n",
       " {'cluster': 9, 'word': 'american'},\n",
       " {'cluster': 9, 'word': 'week'},\n",
       " {'cluster': 9, 'word': 'advertis'},\n",
       " {'cluster': 9, 'word': 'republican'},\n",
       " {'cluster': 9, 'word': 'clinton'}]"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_most_common_words(kmeans_10_cluster3, kmeans_10_cluster3[0], 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reviewing the top words in each of the clusters, they all seem to be political. This leads me to believe that cluster 3 in fact was properly clustered considering the articles were pulled from 2012-2017, right in the midst of the 2016 election period. Going forward, I will add more non-political articles to the dataset in order to adjust the class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bitdeb92ec12ab644c4acad1528174eaa00"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}